{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import models.transformer.fashion_transformer as fashion_t\n",
    "import models.transformer.fashion_encoder as fashion_enc\n",
    "import official.transformer.v2.misc as misc\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "import src.models.transformer.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Before dense\n",
      "(None, None, 2048)\n",
      "After dense\n",
      "(None, None, 512)\n",
      "Model: \"transformer_v2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, None, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 2048)        1050624   \n",
      "_________________________________________________________________\n",
      "encoder_stack_9 (EncoderStac (None, None, 512)         2101760   \n",
      "=================================================================\n",
      "Total params: 4,201,472\n",
      "Trainable params: 4,201,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None, 2048)]      0         \n",
      "_________________________________________________________________\n",
      "transformer_v2 (FashionEncod (None, None, 2048)        4201472   \n",
      "=================================================================\n",
      "Total params: 4,201,472\n",
      "Trainable params: 4,201,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "importlib.reload(fashion_enc)\n",
    "\n",
    "params = misc.get_model_params(\"base\", 1)\n",
    "\n",
    "params.update({\n",
    "    \"feature_dim\": 2048,\n",
    "    \"dtype\": \"float32\",\n",
    "    \"hidden_size\": 512,\n",
    "    \"extra_decode_length\": 0,\n",
    "    \"num_hidden_layers\": 1,\n",
    "    \"num_heads\": 2,\n",
    "    \"max_length\": 10,\n",
    "    \"default_batch_size\": 128,\n",
    "    \"filter_size\": 1024\n",
    "})\n",
    "\n",
    "model = fashion_enc.create_model(params, False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(3, 2, 2048)\n",
      "[[[5.3061867e-01 0.0000000e+00 0.0000000e+00 ... 9.8705864e-01\n",
      "   3.0908227e-02 0.0000000e+00]\n",
      "  [8.4564286e-01 0.0000000e+00 0.0000000e+00 ... 1.5424176e+00\n",
      "   3.2188863e-01 0.0000000e+00]]\n",
      "\n",
      " [[7.1251291e-01 3.5658669e-01 0.0000000e+00 ... 1.1799755e+00\n",
      "   9.2774630e-05 0.0000000e+00]\n",
      "  [1.1838869e+00 7.3470104e-01 0.0000000e+00 ... 1.3230802e+00\n",
      "   0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[7.9162467e-01 3.8026094e-01 0.0000000e+00 ... 9.5547354e-01\n",
      "   7.4496865e-02 0.0000000e+00]\n",
      "  [0.0000000e+00 5.8710259e-01 0.0000000e+00 ... 1.4925238e+00\n",
      "   0.0000000e+00 0.0000000e+00]]]\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None, 2048)]      0         \n",
      "_________________________________________________________________\n",
      "transformer_v2 (FashionEncod (None, None, 2048)        4201472   \n",
      "=================================================================\n",
      "Total params: 4,201,472\n",
      "Trainable params: 4,201,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "input_batch = tf.random.uniform(shape=(3,2,2048))\n",
    "output = model.predict_on_batch(input_batch)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "inputs\n",
      "(None, None, 2048)\n",
      "attention_bias\n",
      "(None, 1, 1, None)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d2163c3074d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m })\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfashion_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\David\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(params, is_train)\u001b[0m\n\u001b[0;32m     62\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"feature_dim\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"inputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m       \u001b[0minternal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFashionTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"transformer_v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minternal_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m       \u001b[0minternal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[1;31m# outputs, scores = ret[\"outputs\"], ret[\"scores\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    841\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n    relative to D:\\:\n\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:150 call  *\n        encoder_outputs = self.encode(inputs, attention_bias, training)\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:195 encode  *\n        return self.encoder_stack(\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:842 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:518 call  *\n        encoder_inputs = self_attention_layer(\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:842 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:453 call  *\n        return x + y\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:899 binary_op_wrapper\n        return func(x, y, name=name)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:1197 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py:549 add_v2\n        \"AddV2\", x=x, y=y, name=name)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:793 _apply_op_helper\n        op_def=op_def)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:548 create_op\n        compute_device)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3429 _create_op_internal\n        op_def=op_def)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1773 __init__\n        control_input_ops)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1613 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 2048 and 512 for 'transformer_v2_1/Transformer/encode/encoder_stack_1/layer_0/self_attention/pre_post_processing_wrapper_5/add' (op: 'AddV2') with input shapes: [?,?,2048], [?,?,512].\n"
     ],
     "ename": "ValueError",
     "evalue": "in converted code:\n    relative to D:\\:\n\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:150 call  *\n        encoder_outputs = self.encode(inputs, attention_bias, training)\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:195 encode  *\n        return self.encoder_stack(\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:842 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:518 call  *\n        encoder_inputs = self_attention_layer(\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:842 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    avid\\Å kola\\outfit-generation\\src\\models\\transformer\\fashion_transformer.py:453 call  *\n        return x + y\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:899 binary_op_wrapper\n        return func(x, y, name=name)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:1197 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py:549 add_v2\n        \"AddV2\", x=x, y=y, name=name)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py:793 _apply_op_helper\n        op_def=op_def)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py:548 create_op\n        compute_device)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:3429 _create_op_internal\n        op_def=op_def)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1773 __init__\n        control_input_ops)\n    rogramy\\Anaconda3\\envs\\outfit-generation\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1613 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 2048 and 512 for 'transformer_v2_1/Transformer/encode/encoder_stack_1/layer_0/self_attention/pre_post_processing_wrapper_5/add' (op: 'AddV2') with input shapes: [?,?,2048], [?,?,512].\n",
     "output_type": "error"
    }
   ],
   "source": [
    "importlib.reload(fashion_t)\n",
    "\n",
    "params = misc.get_model_params(\"base\", 1)\n",
    "\n",
    "params.update({\n",
    "    \"feature_dim\": 2048,\n",
    "    \"dtype\": \"float32\",\n",
    "    \"hidden_size\": 512,\n",
    "    \"extra_decode_length\": 0,\n",
    "    \"num_hidden_layers\": 1,\n",
    "    \"num_heads\": 2,\n",
    "    \"max_length\": 10,\n",
    "    \"default_batch_size\": 128,\n",
    "    \"filter_size\": 1024\n",
    "})\n",
    "\n",
    "model = fashion_t.create_model(params, False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 8, 256)\n",
      "[[[ 0.3721342   0.7154289  -0.06266356 ...  0.7965158  -0.33333158\n",
      "   -0.73048675]\n",
      "  [ 0.3721342   0.7154289  -0.06266356 ...  0.7965158  -0.33333158\n",
      "   -0.73048675]\n",
      "  [ 0.3721342   0.7154289  -0.06266356 ...  0.7965158  -0.33333158\n",
      "   -0.73048675]\n",
      "  ...\n",
      "  [ 0.3721342   0.7154289  -0.06266356 ...  0.7965158  -0.33333158\n",
      "   -0.73048675]\n",
      "  [ 0.3721342   0.7154289  -0.06266356 ...  0.7965158  -0.33333158\n",
      "   -0.73048675]\n",
      "  [ 0.3721342   0.7154289  -0.06266356 ...  0.7965158  -0.33333158\n",
      "   -0.73048675]]\n",
      "\n",
      " [[ 0.80776685  0.04199074 -1.3678492  ...  0.13974936 -1.3057268\n",
      "   -1.5029002 ]\n",
      "  [ 0.80776685  0.04199074 -1.3678492  ...  0.13974936 -1.3057268\n",
      "   -1.5029002 ]\n",
      "  [ 0.80776685  0.04199074 -1.3678492  ...  0.13974936 -1.3057268\n",
      "   -1.5029002 ]\n",
      "  ...\n",
      "  [ 0.80776685  0.04199074 -1.3678492  ...  0.13974936 -1.3057268\n",
      "   -1.5029002 ]\n",
      "  [ 0.80776685  0.04199074 -1.3678492  ...  0.13974936 -1.3057268\n",
      "   -1.5029002 ]\n",
      "  [ 0.80776685  0.04199074 -1.3678492  ...  0.13974936 -1.3057268\n",
      "   -1.5029002 ]]\n",
      "\n",
      " [[ 0.14748147 -0.36441964 -0.27014762 ...  0.7890442  -0.06206196\n",
      "   -1.3667493 ]\n",
      "  [ 0.14748147 -0.36441964 -0.27014762 ...  0.7890442  -0.06206196\n",
      "   -1.3667493 ]\n",
      "  [ 0.14748147 -0.36441964 -0.27014762 ...  0.7890442  -0.06206196\n",
      "   -1.3667493 ]\n",
      "  ...\n",
      "  [ 0.14748147 -0.36441964 -0.27014762 ...  0.7890442  -0.06206196\n",
      "   -1.3667493 ]\n",
      "  [ 0.14748147 -0.36441964 -0.27014762 ...  0.7890442  -0.06206196\n",
      "   -1.3667493 ]\n",
      "  [ 0.14748147 -0.36441964 -0.27014762 ...  0.7890442  -0.06206196\n",
      "   -1.3667493 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.0192876   0.21019636 -0.18677343 ...  0.64026356 -0.37289396\n",
      "   -0.9723649 ]\n",
      "  [ 1.0192876   0.21019636 -0.18677343 ...  0.64026356 -0.37289396\n",
      "   -0.9723649 ]\n",
      "  [ 1.0192876   0.21019636 -0.18677343 ...  0.64026356 -0.37289396\n",
      "   -0.9723649 ]\n",
      "  ...\n",
      "  [ 1.0192876   0.21019636 -0.18677343 ...  0.64026356 -0.37289396\n",
      "   -0.9723649 ]\n",
      "  [ 1.0192876   0.21019636 -0.18677343 ...  0.64026356 -0.37289396\n",
      "   -0.9723649 ]\n",
      "  [ 1.0192876   0.21019636 -0.18677343 ...  0.64026356 -0.37289396\n",
      "   -0.9723649 ]]\n",
      "\n",
      " [[ 1.2259098   0.29313236 -0.3420076  ...  1.1623456  -1.1464028\n",
      "   -1.2901442 ]\n",
      "  [ 1.2259098   0.29313236 -0.3420076  ...  1.1623456  -1.1464028\n",
      "   -1.2901442 ]\n",
      "  [ 1.2259098   0.29313236 -0.3420076  ...  1.1623456  -1.1464028\n",
      "   -1.2901442 ]\n",
      "  ...\n",
      "  [ 1.2259098   0.29313236 -0.3420076  ...  1.1623456  -1.1464028\n",
      "   -1.2901442 ]\n",
      "  [ 1.2259098   0.29313236 -0.3420076  ...  1.1623456  -1.1464028\n",
      "   -1.2901442 ]\n",
      "  [ 1.2259098   0.29313236 -0.3420076  ...  1.1623456  -1.1464028\n",
      "   -1.2901442 ]]\n",
      "\n",
      " [[ 0.9590468  -0.12537736 -0.96861875 ...  0.94336456 -1.4756229\n",
      "   -0.95777786]\n",
      "  [ 0.9590468  -0.12537736 -0.96861875 ...  0.94336456 -1.4756229\n",
      "   -0.95777786]\n",
      "  [ 0.9590468  -0.12537736 -0.96861875 ...  0.94336456 -1.4756229\n",
      "   -0.95777786]\n",
      "  ...\n",
      "  [ 0.9590468  -0.12537736 -0.96861875 ...  0.94336456 -1.4756229\n",
      "   -0.95777786]\n",
      "  [ 0.9590468  -0.12537736 -0.96861875 ...  0.94336456 -1.4756229\n",
      "   -0.95777786]\n",
      "  [ 0.9590468  -0.12537736 -0.96861875 ...  0.94336456 -1.4756229\n",
      "   -0.95777786]]]\n"
     ]
    }
   ],
   "source": [
    "input_batch = tf.random.uniform(shape=(25,8,256))\n",
    "\n",
    "output = model.predict_on_batch(input_batch)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 2. 3.]\n",
      "  [0. 1. 1.]]\n",
      "\n",
      " [[0. 3. 3.]\n",
      "  [0. 0. 0.]]], shape=(2, 2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[False False False]\n",
      "  [ True False False]]\n",
      "\n",
      " [[ True False False]\n",
      "  [ True  True  True]]], shape=(2, 2, 3), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[False False]\n",
      " [False  True]], shape=(2, 2), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[ True  True]\n",
      " [ True False]], shape=(2, 2), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[1 1]\n",
      " [1 0]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Dimension reduction before padding\n",
    "\n",
    "#tensorflow Shape (batch, input_length, feature_dim)\n",
    "test_tensor = tf.constant([1,2,3,0,1,1,0,3,3,0,0,0], shape=(2,2,3), dtype=\"float32\")\n",
    "print(test_tensor)\n",
    "test_tensor = tf.equal(test_tensor,0)\n",
    "print(test_tensor)\n",
    "test_tensor = tf.reduce_all(test_tensor, axis=2)\n",
    "print(test_tensor)\n",
    "test_tensor = tf.math.logical_not(test_tensor)\n",
    "print(test_tensor)\n",
    "test_tensor = tf.cast(test_tensor, dtype=\"int32\")\n",
    "print(test_tensor)\n",
    "test_tensor = tf.reduce_sum(test_tensor)\n",
    "print(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tf.Tensor(\n",
      "[[14.  5. 15.  0.]\n",
      " [ 5.  2.  6.  0.]\n",
      " [15.  6. 18.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[False False False]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True  True  True]], shape=(4, 3), dtype=bool)\n",
      "tf.Tensor([False False False  True], shape=(4,), dtype=bool)\n",
      "tf.Tensor([ True  True  True False], shape=(4,), dtype=bool)\n",
      "tf.Tensor([1. 1. 1. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.33333334, shape=(), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "importlib.reload(metrics)\n",
    "\n",
    "#tensorflow Shape (batch, input_length, feature_dim)\n",
    "pred = tf.constant([1,2,3,0,1,1,0,3,3,0,0,0], shape=(2,2,3), dtype=\"float32\")\n",
    "true = tf.constant([1,2,3,0,1,1,0,3,3,0,0,0], shape=(2,2,3), dtype=\"float32\")\n",
    "\n",
    "loss = metrics.categorical_acc(pred,true)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# pred = tf.constant([1,0,0,0,0,1,0,3,0,0,0,0], shape=(2,2,3), dtype=\"float32\")\n",
    "# true = tf.constant([1,0,0,0,0,1,0,3,0,0,0,0], shape=(2,2,3), dtype=\"float32\")\n",
    "# \n",
    "# loss = metrics.categorical_acc(pred,true)\n",
    "# \n",
    "# print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}