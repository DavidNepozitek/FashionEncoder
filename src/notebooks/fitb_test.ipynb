{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loaded 3076 items\n",
      "Loaded 3076 questions\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-606399c2c3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_fitb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-606399c2c3c3>\u001b[0m in \u001b[0;36mbuild_fitb\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mquestion_item_str\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mq_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey_from_fitb_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_item_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mitem_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_category\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0minput_categories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_category\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('119704139', 1)"
     ],
     "ename": "KeyError",
     "evalue": "('119704139', 1)",
     "output_type": "error"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import src.data.build_dataset as build_dataset\n",
    "\n",
    "dataset_root = \"D:\\David\\Å kola\\RP\\dataset\"\n",
    "test_filename = \"test_no_dup.json\"\n",
    "fitb_filename = \"fill_in_blank_test.json\"\n",
    "output_path = \"fitb_features.tfrecord\"\n",
    "with_features = True\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def key_from_fitb_string(string):\n",
    "    l = string.split(\"_\")\n",
    "    return l[0], int(l[1])\n",
    "    \n",
    "    \n",
    "def build_fitb():\n",
    "    with open(Path(dataset_root, test_filename)) as json_file:\n",
    "        raw_json = json.load(json_file)\n",
    "        print(\"Loaded \" + str(len(raw_json)) + \" items\", flush=True)\n",
    "        items = {}\n",
    "\n",
    "        if with_features:\n",
    "            model = tf.keras.applications.inception_v3.InceptionV3(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "            \n",
    "        # Load all test items into dict\n",
    "        for outfit in raw_json:\n",
    "            set_id = int(outfit[\"set_id\"])\n",
    "\n",
    "            for item in outfit[\"items\"]:\n",
    "                image_path = Path(dataset_root, \"images\", str(set_id), str(item[\"index\"]) + \".jpg\")\n",
    "                if with_features:\n",
    "                    features = build_dataset.extract_features(model, image_path)\n",
    "                    items.update({(set_id, item[\"index\"]): (features, item[\"categoryid\"])})\n",
    "                else:\n",
    "                    with open(image_path, \"rb\") as img_file:\n",
    "                        raw_image = img_file.read()\n",
    "                    items.update({(set_id, item[\"index\"]): (raw_image, item[\"categoryid\"])})\n",
    "    examples = []                \n",
    "    with open(Path(dataset_root, fitb_filename)) as fitb_file:\n",
    "        raw_json = json.load(fitb_file)\n",
    "        print(\"Loaded \" + str(len(raw_json)) + \" questions\", flush=True)\n",
    "        \n",
    "        # Compose questions from FITB file and test items dict\n",
    "        for task in raw_json:\n",
    "            set_id = None\n",
    "            inputs = []\n",
    "            input_categories = []\n",
    "            targets = []\n",
    "            target_categories = []\n",
    "            target_pos = None\n",
    "            \n",
    "            \n",
    "            for question_item_str in task[\"question\"]:\n",
    "                q_key = key_from_fitb_string(question_item_str)\n",
    "                item_features, item_category = items[q_key]\n",
    "                inputs.append(item_features)\n",
    "                input_categories.append(item_category)\n",
    "                set_id = q_key[0]\n",
    "            pos = 0\n",
    "            \n",
    "            for question_item_str in task[\"answers\"]:\n",
    "                q_key = key_from_fitb_string(question_item_str)\n",
    "                if q_key[0] == set_id:\n",
    "                    target_pos = pos\n",
    "                item_features, item_category = items[q_key]\n",
    "                targets.append(item_features)\n",
    "                target_categories.append(item_category)\n",
    "                pos += 1\n",
    "            \n",
    "            if with_features:\n",
    "                question_features = {\n",
    "                    \"input_categories\": tf.train.FeatureList(feature=[_int64_feature(f) for f in input_categories]),\n",
    "                    \"inputs\": tf.train.FeatureList(\n",
    "                        feature=[tf.train.Feature(float_list=tf.train.FloatList(value=f)) for f in inputs]),\n",
    "                    \"target_categories\": tf.train.FeatureList(feature=[_int64_feature(f) for f in target_categories]),\n",
    "                    \"targets\": tf.train.FeatureList(\n",
    "                        feature=[tf.train.Feature(float_list=tf.train.FloatList(value=f)) for f in targets])\n",
    "                }\n",
    "            else:\n",
    "                question_features = {\n",
    "                    \"input_categories\": tf.train.FeatureList(feature=[_int64_feature(f) for f in input_categories]),\n",
    "                    \"inputs\": tf.train.FeatureList(feature=[_bytes_feature(f) for f in inputs]),\n",
    "                    \"target_categories\": tf.train.FeatureList(feature=[_int64_feature(f) for f in target_categories]),\n",
    "                    \"targets\": tf.train.FeatureList(feature=[_bytes_feature(f) for f in targets])\n",
    "                }\n",
    "            feature_lists = tf.train.FeatureLists(feature_list=question_features)\n",
    "            context = {\n",
    "                \"target_position\": target_pos\n",
    "            }\n",
    "            context = tf.train.Features(feature=context)\n",
    "            example = tf.train.SequenceExample(feature_lists=feature_lists, context=context)\n",
    "            examples.append(example.SerializeToString())\n",
    "        return examples\n",
    "\n",
    "\n",
    "examples = build_fitb()\n",
    "with tf.io.TFRecordWriter(output_path) as writer:\n",
    "    for i in range(len(examples)):\n",
    "        writer.write(examples[i])\n",
    "        \n",
    "print(\"Saved the fitb successfully\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}